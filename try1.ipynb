{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIynlK9lXaAa",
        "outputId": "555cc1da-8c51-4424-d2ac-85f6d3542e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X39pAZt2XcD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/Train'\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/Test'\n",
        "\n",
        "train_count = sum(len(files) for _, _, files in os.walk(train_path))\n",
        "test_count = sum(len(files) for _, _, files in os.walk(test_path))\n",
        "\n",
        "print(f\"Number of train images: {train_count}\")\n",
        "print(f\"Number of test images: {test_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPBQe6CRXcHR",
        "outputId": "983bf262-a405-4cae-c499-d9f8f6e0c924"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train images: 12012\n",
            "Number of test images: 3001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "print(\"Number of train data:\", len(train_data))\n",
        "print(\"Number of test data:\", len(test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZIgaNT_kcuP",
        "outputId": "64d886be-dfc7-4a22-a33e-3db01bd8afe1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train data: 11999\n",
            "Number of test data: 2999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qqVFCU4l745",
        "outputId": "f7dcd79c-b5f5-4c91-a7f3-4ca8585ca11a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       255  255.1  255.2  255.3  255.4  255.5  255.6  255.7  255.8  255.9  \\\n",
            "0      255    255    255    255    255    255    255    255    235     67   \n",
            "1      255    255    255    255    255    255    255    255    255    255   \n",
            "2      255    255    255    255    255    255    255    255    255    255   \n",
            "3      255    255    255    255    255    255    255    255    255    255   \n",
            "4        0      0    255    255    255    255    255    255    255    255   \n",
            "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "11994  127    127    127    127    127    127    127    127    127    127   \n",
            "11995  126    146     99      0      0     59    145    129    126    127   \n",
            "11996  127    127    127    127    127    127    127    127    127    127   \n",
            "11997  127    127    127    127    127    127    127    127    127    127   \n",
            "11998  127    127    127    127    127    127    127    127    127    127   \n",
            "\n",
            "       ...  255.1614  255.1615  255.1616  255.1617  255.1618  255.1619  \\\n",
            "0      ...       255       255        10       255       254       255   \n",
            "1      ...       255       255       255       255       255         0   \n",
            "2      ...       255       255       255       202         4       141   \n",
            "3      ...       207        83       251       243       255       255   \n",
            "4      ...       255       253       254       255       255       255   \n",
            "...    ...       ...       ...       ...       ...       ...       ...   \n",
            "11994  ...       127       127       127       127       127       127   \n",
            "11995  ...       127       127       127       127       127       127   \n",
            "11996  ...       127       127       127       127       127       127   \n",
            "11997  ...       127       127       127       127       127       127   \n",
            "11998  ...       127       127       127       127       127       127   \n",
            "\n",
            "       255.1620  255.1621  255.1622  255.1623  \n",
            "0           255       255       255       255  \n",
            "1             0       116       255       255  \n",
            "2           255       255       255       255  \n",
            "3           255       255       255       255  \n",
            "4           255       255       255       255  \n",
            "...         ...       ...       ...       ...  \n",
            "11994       127       127       127       127  \n",
            "11995       127       127       127       127  \n",
            "11996       127       127       127       127  \n",
            "11997       127       127       127       127  \n",
            "11998       127       127       127       127  \n",
            "\n",
            "[11999 rows x 2400 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "_wKqst5kwWqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the training dataset\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Drop rows with missing values from the training data\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Find common columns between the training and test sets\n",
        "common_columns = train_data.columns.intersection(test_data.columns)\n",
        "\n",
        "# Split the training dataset into features (X) and target variable (y)\n",
        "X_train = train_data[common_columns].iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "\n",
        "# Split the test dataset into features (X) and target variable (y)\n",
        "X_test = test_data[common_columns].iloc[:, :-1]  # Ensure test set columns match the training set columns\n",
        "y_test = test_data.iloc[:, -1]\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRIpX36emUnd",
        "outputId": "74d1c92e-79aa-49d4-903a-dc04b49bb9b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8919639879959986\n",
            "Precision: 0.9009410824688645\n",
            "Recall: 0.8919639879959986\n",
            "F1 Score: 0.8557802373554627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regession"
      ],
      "metadata": {
        "id": "1dFvswONwSPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the training dataset\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Drop rows with missing values from the training data\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Find common columns between the training and test sets\n",
        "common_columns = train_data.columns.intersection(test_data.columns)\n",
        "\n",
        "# Split the training dataset into features (X) and target variable (y)\n",
        "X_train = train_data[common_columns].iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "\n",
        "# Split the test dataset into features (X) and target variable (y)\n",
        "X_test = test_data[common_columns].iloc[:, :-1]  # Ensure test set columns match the training set columns\n",
        "y_test = test_data.iloc[:, -1]\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "logreg_classifier = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the classifier on the scaled training data\n",
        "logreg_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the labels for the scaled test set\n",
        "y_pred = logreg_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GEuQxL3pd5C",
        "outputId": "b80bf04e-9cc2-4e39-9ae1-fde286484621"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7625875291763922\n",
            "Precision: 0.8594421133239835\n",
            "Recall: 0.7625875291763922\n",
            "F1 Score: 0.7840144690157969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "KQPuskmlwOI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the training dataset\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Drop rows with missing values from the training data\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Find common columns between the training and test sets\n",
        "common_columns = train_data.columns.intersection(test_data.columns)\n",
        "\n",
        "# Split the training dataset into features (X) and target variable (y)\n",
        "X_train = train_data[common_columns].iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "\n",
        "# Split the test dataset into features (X) and target variable (y)\n",
        "X_test = test_data[common_columns].iloc[:, :-1]  # Ensure test set columns match the training set columns\n",
        "y_test = test_data.iloc[:, -1]\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcGyd-zFvu35",
        "outputId": "b2e2c6c0-7587-411d-aec1-993e0acd90a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.895298432810937\n",
            "Precision: 0.8939032342221925\n",
            "Recall: 0.895298432810937\n",
            "F1 Score: 0.8473092419123334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNB"
      ],
      "metadata": {
        "id": "Lvm4lTqN7SOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the training dataset\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Drop rows with missing values from the training data\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Find common columns between the training and test sets\n",
        "common_columns = train_data.columns.intersection(test_data.columns)\n",
        "\n",
        "# Split the training dataset into features (X) and target variable (y)\n",
        "X_train = train_data[common_columns].iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "\n",
        "# Split the test dataset into features (X) and target variable (y)\n",
        "X_test = test_data[common_columns].iloc[:, :-1]  # Ensure test set columns match the training set columns\n",
        "y_test = test_data.iloc[:, -1]\n",
        "\n",
        "# Initialize the GNB classifier\n",
        "gnb_classifier = GaussianNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "gnb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = gnb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WqPcW9mqmvE",
        "outputId": "791c75d2-c805-4df3-d31f-d7a244a9777a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6382127375791931\n",
            "Precision: 0.8981307748333569\n",
            "Recall: 0.6382127375791931\n",
            "F1 Score: 0.7231215528775626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "5w4fxLTJ7j16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the training dataset\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Drop rows with missing values from the training data\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Find common columns between the training and test sets\n",
        "common_columns = train_data.columns.intersection(test_data.columns)\n",
        "\n",
        "# Split the training dataset into features (X) and target variable (y)\n",
        "X_train = train_data[common_columns].iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "\n",
        "# Split the test dataset into features (X) and target variable (y)\n",
        "X_test = test_data[common_columns].iloc[:, :-1]  # Ensure test set columns match the training set columns\n",
        "y_test = test_data.iloc[:, -1]\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGYovx_G7Q2w",
        "outputId": "72f9e257-e3c9-4d76-e405-43380e4d412f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8609536512170723\n",
            "Precision: 0.8601157324081685\n",
            "Recall: 0.8609536512170723\n",
            "F1 Score: 0.8160152224663493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEcesion tree"
      ],
      "metadata": {
        "id": "vGixC1LE8Kxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the training dataset\n",
        "train_path = '/content/drive/MyDrive/bangla_Handwritten_data/train.csv'\n",
        "train_data = pd.read_csv(train_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_path = '/content/drive/MyDrive/bangla_Handwritten_data/test.csv'\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Drop rows with missing values from the training data\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Find common columns between the training and test sets\n",
        "common_columns = train_data.columns.intersection(test_data.columns)\n",
        "\n",
        "# Split the training dataset into features (X) and target variable (y)\n",
        "X_train = train_data[common_columns].iloc[:, :-1]\n",
        "y_train = train_data.iloc[:, -1]\n",
        "\n",
        "# Split the test dataset into features (X) and target variable (y)\n",
        "X_test = test_data[common_columns].iloc[:, :-1]  # Ensure test set columns match the training set columns\n",
        "y_test = test_data.iloc[:, -1]\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and f1 score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LKGnTU17lky",
        "outputId": "d570173b-afb0-4348-f8f6-74868a85e4f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6705568522840947\n",
            "Precision: 0.8754644558161778\n",
            "Recall: 0.6705568522840947\n",
            "F1 Score: 0.7517682177779309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwEllZwh8UWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}